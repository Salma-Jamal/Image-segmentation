{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4740c4-1360-479f-b992-c5d0103ed25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run only once)\n",
    "!pip install torch torchvision tqdm seaborn sklearn transformers wandb argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60db22c-c315-4c72-9b0a-cfb566a32a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import (\n",
    "    RandomResizedCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    ColorJitter,\n",
    "    RandomGrayscale,\n",
    "    RandomApply,\n",
    "    Compose,\n",
    "    GaussianBlur,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    CenterCrop,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import wandb\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e193ea-a92d-42d9-8e93-101cbdf4b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'DEVICE: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76561646-1638-4d9e-8055-8c06f91ffb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'simclr_rad-dino_pos-pairs_aug-pairs_100_epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d596be-52d5-40a5-acba-2611b435703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 70  # Adjust as needed\n",
    "        self.patience = 30\n",
    "        self.dropout_p = 0.3\n",
    "        self.image_shape = [256, 256]\n",
    "        self.kernel_size = [21, 21]  # For the transforms, 10% of image size\n",
    "        self.embedding_size = 128\n",
    "        self.scheduler_step_size = 70\n",
    "        self.scheduler_gamma = 0.1\n",
    "        self.weight_decay = 1e-5\n",
    "        self.max_norm = 1.0  # Gradient clipping\n",
    "        self.temperature = 2.0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_path = f\"/home/saahmed/scratch/projects/Image-segmentation/retrieval/checkpoints/{run_name}\"\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "        self.best_model_path = os.path.join(self.base_path, \"best_model.pth\")\n",
    "        self.last_model_path = os.path.join(self.base_path, \"last_model.pth\")\n",
    "        self.learning_plot_path = os.path.join(self.base_path, \"learning_curves.png\")\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a487964d-1db5-41c7-ad76-8996a9183339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_API_KEY\"] = \"4f8dccbaced16f201316dd4113139739694dfd3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64fd7ff6-39ba-40f4-9363-21215b195a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msalmagg\u001b[0m (\u001b[33mmy_research_projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/saahmed/Notebooks/wandb/run-20250212_080749-pg8san7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/my_research_projects/simclr-training/runs/pg8san7s' target=\"_blank\">simclr_rad-dino_pos-pairs_aug-pairs</a></strong> to <a href='https://wandb.ai/my_research_projects/simclr-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/my_research_projects/simclr-training' target=\"_blank\">https://wandb.ai/my_research_projects/simclr-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/my_research_projects/simclr-training/runs/pg8san7s' target=\"_blank\">https://wandb.ai/my_research_projects/simclr-training/runs/pg8san7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Initialize wandb and log the configuration parameters.\n",
    "# wandb.init(\n",
    "#     project=\"simclr-training\",\n",
    "#     name=\"simclr_rad-dino_pos-pairs_aug-pairs\",\n",
    "#     config={\n",
    "#         \"learning_rate\": config.learning_rate,\n",
    "#         \"num_epochs\": config.num_epochs,\n",
    "#         \"batch_size\": config.batch_size,\n",
    "#         \"dropout_p\": config.dropout_p,\n",
    "#         \"image_shape\": config.image_shape,\n",
    "#         \"embedding_size\": config.embedding_size,\n",
    "#         \"scheduler_step_size\": config.scheduler_step_size,\n",
    "#         \"scheduler_gamma\": config.scheduler_gamma,\n",
    "#         \"weight_decay\": config.weight_decay,\n",
    "#         \"max_norm\": config.max_norm,\n",
    "#         \"temperature\": config.temperature,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Optionally, add the config to wandb for reference\n",
    "# wandb_config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21d5b89-43b3-4d8c-9950-5b385b0a1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34med\u001b[0m/  \u001b[01;34mes\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f33090a-9f37-45e1-ae10-ad0ea10715aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  1902\n",
      "Images in train split:  1331\n",
      "Images in validation split:  571\n",
      "Batches in TRAIN:  30\n",
      "Batches in VAL:  10\n"
     ]
    }
   ],
   "source": [
    "def convert_to_rgb(img):\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "class AugmentationSequenceType(Enum):\n",
    "    temp = \"temp\"\n",
    "    normal = \"normal\"\n",
    "\n",
    "augmentation_sequence_map = {\n",
    "    AugmentationSequenceType.temp.value: transforms.Compose([\n",
    "        transforms.Resize((config.image_shape[0], config.image_shape[1])),\n",
    "        transforms.Lambda(convert_to_rgb),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.Lambda(lambda img: transforms.functional.adjust_contrast(img, contrast_factor=random.uniform(1, 1.3))),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    AugmentationSequenceType.normal.value: transforms.Compose([\n",
    "        transforms.Resize((config.image_shape[0], config.image_shape[1])),\n",
    "        transforms.Lambda(convert_to_rgb),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class ContrastiveLearningViewGenerator(object):\n",
    "    def __init__(self, base_transform, normal_transform, n_views=2):\n",
    "        self.base_transform = base_transform\n",
    "        self.normal_transform = normal_transform\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if random.random() < 0.5:\n",
    "            views = [self.base_transform(x) for _ in range(self.n_views)]\n",
    "        else:\n",
    "            views = [self.normal_transform(x), self.base_transform(x)]\n",
    "        return views\n",
    "\n",
    "class CombinedContrastiveDataset(Dataset):\n",
    "    def __init__(self, list_images, positive_pairs, base_transform, normal_transform):\n",
    "        self.list_images = list_images\n",
    "        self.positive_pairs = positive_pairs\n",
    "        self.all_images = self.positive_pairs + self.list_images\n",
    "        self.base_transform = base_transform\n",
    "        self.normal_transform = normal_transform\n",
    "        self.view_generator = ContrastiveLearningViewGenerator(base_transform, normal_transform, n_views=2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_images) + len(self.positive_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.positive_pairs):\n",
    "            img_path1, img_path2 = self.all_images[idx]\n",
    "            img1 = Image.open(img_path1)\n",
    "            img2 = Image.open(img_path2)\n",
    "            img1 = self.normal_transform(img1) \n",
    "            img2 = self.normal_transform(img2)\n",
    "            return [img1, img2]\n",
    "        else:\n",
    "            img_path = self.all_images[idx]\n",
    "            img = Image.open(img_path)\n",
    "            views = self.view_generator(img)\n",
    "            return views\n",
    "\n",
    "# Prepare training image list\n",
    "images_list_train = []\n",
    "for i in ['es', 'ed']:\n",
    "    path = f'/home/saahmed/scratch/projects/Image-segmentation/datasets/ACDC/processed_data/Training/{i}/images/'\n",
    "    images = [os.path.join(path, fname) for fname in os.listdir(path)]\n",
    "    images_list_train += images\n",
    "\n",
    "def train_val_test_split(list_filenames, train_size=0.7):\n",
    "    list_filenames_train, list_filenames_val = train_test_split(\n",
    "        list_filenames,\n",
    "        train_size=train_size,\n",
    "        shuffle=True,\n",
    "        random_state=42)\n",
    "    return list_filenames_train, list_filenames_val\n",
    "\n",
    "list_images = images_list_train\n",
    "list_images_train, list_images_val = train_val_test_split(list_images)\n",
    "\n",
    "print(\"Total number of images: \", len(list_images))\n",
    "print(\"Images in train split: \", len(list_images_train))\n",
    "print(\"Images in validation split: \", len(list_images_val))\n",
    "\n",
    "def create_positive_pairs(images):\n",
    "    file_list = sorted(images)\n",
    "    pattern = re.compile(r'patient(\\d+)_frame(\\d+)_slice_(\\d+)\\.png')\n",
    "    groups = defaultdict(list)\n",
    "    for path in file_list:\n",
    "        filename = os.path.basename(path)\n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            patient = match.group(1)\n",
    "            frame = match.group(2)\n",
    "            slice_num = int(match.group(3))\n",
    "            key = (patient, frame)\n",
    "            groups[key].append((slice_num, path))\n",
    "        else:\n",
    "            print(f\"File {path} does not match the expected pattern.\")\n",
    "    \n",
    "    positive_pairs = []\n",
    "    for key, slices in groups.items():\n",
    "        slices.sort(key=lambda x: x[0])\n",
    "        for i in range(len(slices) - 1):\n",
    "            img1 = slices[i][1]\n",
    "            img2 = slices[i + 1][1]\n",
    "            if (eval(img1.split('_')[-1].replace('.png','')) + 1) == eval(img2.split('_')[-1].replace('.png','')):\n",
    "                positive_pairs.append((img1, img2))\n",
    "    return positive_pairs\n",
    "\n",
    "pos_pairs_train = create_positive_pairs(list_images_train)\n",
    "pos_pairs_val = create_positive_pairs(list_images_val)\n",
    "\n",
    "output_shape = config.image_shape \n",
    "base_transforms = augmentation_sequence_map[AugmentationSequenceType.temp.value]\n",
    "normal_transforms = augmentation_sequence_map[AugmentationSequenceType.normal.value]\n",
    "\n",
    "image_ds_train = CombinedContrastiveDataset(\n",
    "    list_images=list_images_train,\n",
    "    positive_pairs=pos_pairs_train,\n",
    "    base_transform=base_transforms,\n",
    "    normal_transform=normal_transforms)\n",
    "\n",
    "image_ds_val = CombinedContrastiveDataset(\n",
    "    list_images=list_images_val,\n",
    "    positive_pairs=pos_pairs_val,\n",
    "    base_transform=base_transforms,\n",
    "    normal_transform=normal_transforms)\n",
    "\n",
    "BATCH_SIZE = config.batch_size\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    image_ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    image_ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Batches in TRAIN: \", len(train_loader))\n",
    "print(\"Batches in VAL: \", len(val_loader))\n",
    "# Note: If you have a test_loader, print it similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836b8b31-652e-4914-965e-e6fb0bc6da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples in TRAIN:  2161\n",
      "samples in VAL:  719\n"
     ]
    }
   ],
   "source": [
    "print(\"samples in TRAIN: \", len(image_ds_train))\n",
    "print(\"samples in VAL: \", len(image_ds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601ca959-0343-4a38-9583-df8fc13afb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Dinov2Model\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, dropout_p=0.5, embedding_size=128, freeze=False, linear_eval=False):\n",
    "        super().__init__()\n",
    "        self.linear_eval = linear_eval\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Load the DINOv2 model (you can change to any pretrained model)\n",
    "        self.encoder = Dinov2Model.from_pretrained('microsoft/rad-dino')\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Assuming DINOv2 has an embedding dimension of 768\n",
    "            nn.Dropout(p=self.dropout_p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.linear_eval:\n",
    "            x = torch.cat(x, dim=0)  # Concatenate the two views\n",
    "        outputs = self.encoder(x)\n",
    "        encoding = outputs.last_hidden_state[:, 0]  # Extract the [CLS] token representation\n",
    "        projection = self.projection(encoding)\n",
    "        return projection\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    model.encoder.save_pretrained(save_path)\n",
    "    torch.save(model.projection.state_dict(), os.path.join(save_path, 'projection_head.pth'))\n",
    "\n",
    "def load_model(model_class, load_path, device):\n",
    "    encoder = Dinov2Model.from_pretrained(load_path)\n",
    "    model = model_class()\n",
    "    model.encoder = encoder\n",
    "    projection_head_path = os.path.join(load_path, 'projection_head.pth')\n",
    "    model.projection.load_state_dict(torch.load(projection_head_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "def plot_training(train_loss_history, save_path, val_loss_history=None):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    if val_loss_history is not None:\n",
    "        plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def contrastrive_loss(features, config):\n",
    "    \"\"\"NT-Xent (Normalized Temperature-Scaled Cross Entropy) Loss,\n",
    "    aka. Contrastive Loss, used in the SimCLR paper.\n",
    "\n",
    "    IMPORTANT NOTE: We don't really return the loss, but the logits\n",
    "    and the (synthetic) labels to compute it with CrossEntropyLoss!\n",
    "\n",
    "    The main idea behind SimCLR and contrastive learning is to learn\n",
    "    representations that are close for positive pairs and far for negative pairs.\n",
    "    In the case of SimCLR, a positive pair is two different augmentations\n",
    "    of the same image, and a negative pair is two augmentations\n",
    "    of two different images.\n",
    "\n",
    "    How NT-Xent works:\n",
    "    - Compute the cosine similarity between the representations\n",
    "    of all pairs of images in the batch.\n",
    "    - Apply a softmax to these similarities, but treat the similarity\n",
    "    of each image with its positive pair as the correct class.\n",
    "    This means that for each image, the goal is to make the\n",
    "    softmax probability of its positive pair as high as possible,\n",
    "    and the softmax probabilities of its negative pairs as low as possible.\n",
    "    - Compute the cross entropy between these softmax probabilities\n",
    "    and the true labels (which have a 1 for the positive pair\n",
    "    and 0 for the negative pairs).\n",
    "    - The temperature parameter scales the similarities before the softmax.\n",
    "    A lower temperature makes the softmax output more peaky\n",
    "    (i.e., the highest value will be much higher than the others,\n",
    "    and the lower values will be closer to zero),\n",
    "    while a higher temperature makes the softmax output more uniform.\n",
    "\n",
    "    Args:\n",
    "        projections: cat(z1, z2)\n",
    "        z1: The projection of the first branch/view\n",
    "        z2: The projeciton of the second branch/view\n",
    "\n",
    "    Returns:\n",
    "        the NTxent loss\n",
    "\n",
    "    Notes on the shapes:\n",
    "        inputs to model (views): [(B, C, W, H), (B, C, W, H)]\n",
    "            B: batch size\n",
    "            C: channels\n",
    "            W: width\n",
    "            H: height\n",
    "            E: embedding size\n",
    "        outputs from model (projections): [2*B, E]\n",
    "        LABELS: [2*B, 2*B]\n",
    "        features = outputs from model: [2*B, E]\n",
    "        mask: [2*B, 2*B]\n",
    "        similarity_matrix: [2*B, 2*B-1]\n",
    "        positives: [2*B, 1]\n",
    "        negatives: [2*B, 2*B-2]\n",
    "        logits: [2*B, 2*B-1]\n",
    "        labels: [2*B]\n",
    "    \"\"\"\n",
    "    # FIXME: Refactor: take config out and pass necessary params, remove capital variables, etc.\n",
    "    # FIXME: convert into class\n",
    "    BATCH_SIZE = config.batch_size\n",
    "    DEVICE = config.device\n",
    "    TEMPERATURE = config.temperature\n",
    "\n",
    "    LABELS = torch.cat([torch.arange(BATCH_SIZE) for i in range(2)], dim=0)\n",
    "    LABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float() # Creates a one-hot with broadcasting\n",
    "    LABELS = LABELS.to(DEVICE) # 2*B, 2*B\n",
    "\n",
    "    similarity_matrix = torch.matmul(features, features.T) # 2*B, 2*B\n",
    "    # discard the main diagonal from both: labels and similarities matrix\n",
    "    mask = torch.eye(LABELS.shape[0], dtype=torch.bool).to(DEVICE)\n",
    "    # ~mask is the negative of the mask\n",
    "    # the view is required to bring the matrix back to shape\n",
    "    labels = LABELS[~mask].view(LABELS.shape[0], -1) # 2*B, 2*B-1\n",
    "    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1) # 2*B, 2*B-1\n",
    "\n",
    "    # select and combine multiple positives\n",
    "    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1) # 2*B, 1\n",
    "\n",
    "    # select only the negatives\n",
    "    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1) # 2*B, 2*B-2\n",
    "\n",
    "    logits = torch.cat([positives, negatives], dim=1) # 2*B, 2*B-1\n",
    "    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    logits = logits / TEMPERATURE\n",
    "\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d8bb35-f24f-4391-94d3-26cb61bb798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimCLR(dropout_p=config.dropout_p, embedding_size=config.embedding_size).to(config.device)\n",
    "criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "\n",
    "# Optionally, let wandb watch the model (logs gradients and parameters)\n",
    "# wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2d9578-a528-46f5-ae4e-4be754fe6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, config):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for views in val_loader:\n",
    "            projections = model([view.to(config.device) for view in views])\n",
    "            logits, labels = contrastrive_loss(projections, config)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, output_freq=2, debug=False):\n",
    "    model = model.to(config.device)\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    total_batches = len(train_loader)\n",
    "    print_every = total_batches // output_freq\n",
    "\n",
    "    for epoch in tqdm(range(config.num_epochs)):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for i, views in enumerate(train_loader):\n",
    "            projections = model([view.to(config.device) for view in views])\n",
    "            logits, labels = contrastrive_loss(projections, config)\n",
    "            if debug and (torch.isnan(logits).any() or torch.isinf(logits).any()):\n",
    "                print(\"[WARNING]: large logits\")\n",
    "                logits = logits.clamp(min=-10, max=10)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_norm)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        val_loss = validate(model, val_loader, criterion, config)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # # Log metrics to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train_loss\": train_loss,\n",
    "        #     \"val_loss\": val_loss,\n",
    "        #     \"learning_rate\": current_lr,\n",
    "        #     \"epoch_time\": epoch_time\n",
    "        # })\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {train_loss}, Val Loss: {val_loss}, Time: {epoch_time:.2f}s, LR: {current_lr}\")\n",
    "\n",
    "        # Save the last model checkpoint locally and log it as an artifact if needed.\n",
    "        save_model(model, config.last_model_path)\n",
    "        # artifact = wandb.Artifact(\"last-model\", type=\"model\", metadata={\"epoch\": epoch+1})\n",
    "        # artifact.add_dir(config.last_model_path)\n",
    "        # wandb.log_artifact(artifact, aliases=[\"latest\"])\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config.best_model_path)\n",
    "\n",
    "            # # Create a wandb artifact and log the best model checkpoint\n",
    "            # artifact = wandb.Artifact(\"best-model\", type=\"model\", metadata={\"epoch\": epoch+1})\n",
    "            # artifact.add_dir(config.best_model_path)\n",
    "            # wandb.log_artifact(artifact, aliases=[\"latest\"])\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= config.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db6120d6-827f-43c2-940f-f59aaf7d2e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                     | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 4.8712455113728845, Val Loss: 4.250300025939941, Time: 102.70s, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆ                                                                                                         | 1/100 [02:49<4:39:13, 169.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss_history, val_loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, config, output_freq, debug)\u001b[0m\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mmax_norm)\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3d264-af81-4cd6-944d-2c44250a4532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
